{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical birds eye view of the contents in an AiiDAdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first of two deliverable for the SiSc-Lab2020 project.\n",
    "\n",
    "Authors = Miao Wang(a - e), Zhipeng Tan(f - i)\n",
    "\n",
    "Supervisors: Jens Bröder, Dr. Daniel Wortmann, Johannes Wasmer, Prof. Dr. Stefan Blügel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions by supervisors\n",
    "\n",
    "## Jens\n",
    "a= \"\"\"\n",
    "You have to implement this notebook.\n",
    "\n",
    "In the end only text (markdown) cells and output results of code cells should be seen if one hides the code cells (hide_code extension).\n",
    "\n",
    "That can easily exported into a nice pdf file (google it, probably will find sth with `nbconvert`).\n",
    "\n",
    "Also the amount of python code in this notebook should be minimal.\n",
    "\n",
    "Rather, export the functions you use to python file(s) and import them here (hide complexity).\n",
    "\n",
    "Optional dump query results in a file, from which results will be reread for speed, i.e cache results.\n",
    "\"\"\"\n",
    "\n",
    "## Johannes\n",
    "a = '''\n",
    "After talking with Jens about it, here are some additional thoughts on the code structure and implementation, for both deliverables.\n",
    "\n",
    "The **primary** goal is of course that the code should work, produce nice output (and helpful error messages), obviously.\n",
    "\n",
    "The **secondary** goal is speed. How long do you expect your code to run on a dataset of a given size? Are there multiple paths to a goal, but with differing performance?\n",
    "\n",
    "You can break the runtime down into several steps: data acquisition, data transformation (or preprocessing), data analysis, data visualization. In this project, we will rename/replace these steps to: **querying, de-/serialization, analysis=visualization**.\n",
    "\n",
    "**Querying the database.** Performance considerations:\n",
    "- Performance measurement: use the magics `%time` and `%timeit`.\n",
    "- Query evaluations: queries (in general) use 'lazy evaluation'.\n",
    "  - *Query building* methods build the query but do not execute it. These are chainable methods like `append()`, `get_outgoing()`, etc.\n",
    "  - *Query execution* methods send the query to the database to be evaluated. There are two kinds:\n",
    "    - non-iterator methods: e.g. `all()`, `first()`, etc. These return a result `list`: all items are loaded into memory.\n",
    "    - iterator methods: e.g. `iterall()`, `iterdict()`. These return a result `Generator`: only one item at a time is loaded into memory.\n",
    "    \n",
    "**De-/serialization**, i.e. writing and reading it to/from a file. *Keep in mind: if you come to the conclusion this is unnecessary, then justify it!* Considerations:\n",
    "- Necessity: we assume 'yes'. So you need serialization/deserialization routine(s).\n",
    "- Code design: we recommend to write a serializer that moves *all* data needed from aiida to file (perform query & serialization). Then the visualization methods are decoupled from aiida and load data from that file. Advantages: a) only needs to be called when data in database changed, b) similar queries for different visualizations can be performed only once. One design option is this:\n",
    "  ```python\n",
    "  serialize = sisclab.Serializer(profile)\n",
    "  serialize.to_file(filepath)\n",
    "  visualize = sisclab.Visualizer(filepath)\n",
    "  visualize.histogram(cumulative=True, plot_options)\n",
    "  # plots histogram\n",
    "  ```\n",
    "- Serialization format: there are two practical options (maybe more):\n",
    "  - `dict`: tree-like. JSON format. One `dict` per file. choose key-value (nested?) based on use-case. in general, `uuid` is a good key.\n",
    "  - `pandas.Dataframe`: could be preferrable in some cases.\n",
    "- Serialization location:\n",
    "  - one file or several files?\n",
    "  - we recommend to de/serialize from/to `sisclab/data/` folder. It is included in the project's `.gitignore` file, so nothing in it gets committed to/from git (git is for code, not for data; the code generates the data).\n",
    "- Transformation:\n",
    "  - if needed, decide where to put needed data transformations (before serialization or after deserialization) to minimize them.\n",
    "- Deserialization: \n",
    "  - a class (as above) might help to define the deserialization format only once for all visualization methods.\n",
    "  \n",
    "\n",
    "**Visualization**:\n",
    "- Prefer `bokeh` to `matplotlib` or other libs wherever possible, unless you have a good justification.\n",
    "- In `D1`, static plots are okay, interactive plots are a bonus.\n",
    "- Lists results (when plot is overkill) will look nicer in a notebook if they are a `pandas.Series` or `pandas.Dataframes`.\n",
    "- Think about function signatures. Can you generalize them to make a nice interface? For example, a signature for SubtaskD1.c might look like this:\n",
    "  ```python\n",
    "  def node_type_summary(user_list : list = [], node_basetype : Node = Data,\n",
    "                        chart_type : bokeh.chart_type = bokeh.pie_chart, plot : bool = True):\n",
    "    \"\"\"\n",
    "    :param user_list: list of users. empty list = all users = default.\n",
    "    :param node_basetype: subdivides chart into subtypes. Valid base type examples: ProcessNode, CalculationNode, WorkflowNode, Data, ArrayData.\n",
    "    :param chart_type: bokeh visualization type. pie chart = default.\n",
    "    :param plot: True: show plot, don't return data. False: don't plot, return data.\n",
    "    :return: stats: a dictionary {node_subtype : node_count}, insertion-order sorted in descending order.\n",
    "    :rtype: dict.\n",
    "    \"\"\"\n",
    "  ```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magics:\n",
    "# # autoreload imports. \n",
    "# # intent: if i change sth in import, i don't have to restart kernel. enable only for development.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# # choose matplotlib backend. backend 'notebook' allows interactive plots if your env allows it.\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python imports:\n",
    "import time\n",
    "import json\n",
    "#from pprint import pprint\n",
    "\n",
    "#%pylab inline\n",
    "#figuresize=(18, 4)\n",
    "from collections import Counter\n",
    "from math import pi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from bokeh.io import output_file,output_notebook, show\n",
    "from bokeh.layouts import column\n",
    "from bokeh.palettes import Category20,Category20c,Spectral11\n",
    "from bokeh.plotting import figure,ColumnDataSource\n",
    "from bokeh.transform import cumsum\n",
    "from bokeh.models import Legend,LegendItem,HoverTool,ColumnDataSource\n",
    "\n",
    "# aiida imports:\n",
    "from aiida import load_profile\n",
    "profile = load_profile()\n",
    "\n",
    "# ggf add futher imports\n",
    "from aiida.orm import QueryBuilder as QB\n",
    "from aiida.orm import QueryBuilder\n",
    "from aiida.orm import WorkflowNode\n",
    "from aiida.orm import load_node, Node, Group, Computer,Dict\n",
    "from aiida.orm import User, CalcJobNode, Code, StructureData, ProcessNode\n",
    "from aiida.plugins import DataFactory\n",
    "from aiida.common.constants import elements as PeriodicTableElements\n",
    "\n",
    "# project imports:\n",
    "#import helpers\n",
    "# if this does not work, do a `pip install -e .` in the aiida-jutools head folder\n",
    "from aiida_jutools.sisc_lab import helpers\n",
    "\n",
    "\n",
    "import aiida_jutools.sisc_lab.util.data_visu as DV\n",
    "import aiida_jutools.sisc_lab.util.serialization as SR\n",
    "from aiida_jutools.sisc_lab.util.data_visu import AnalyseStructureElements,ShowElements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (example:)\n",
    "helpers.print_bold(f\"This notebook/dashboard will visualize the contents from the database of profile {profile.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubtaskD1.a: Node information\n",
    "#Task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for all nodes\n",
    "print('Information on nodes in the DB: \\n')\n",
    "now = time.strftime(\"%c\")\n",
    "print('last executed on {}'.format(now))\n",
    "q = QB()\n",
    "q.append(Node, project=['id', 'ctime', 'mtime', 'node_type'], tag='node')\n",
    "q.append(User, with_node='node', project='email')\n",
    "# TODO: execute query here\n",
    "t = time.time()\n",
    "res = q.all()\n",
    "elapsed = time.time() - t\n",
    "totalnodes = len(res)\n",
    "print(\"Total number of nodes in the database: {} (retrieved in {} s.)\".format(totalnodes, elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubtaskD1.b: Users\n",
    "a = '''\n",
    "Task: print out a list of Users and how many nodes belong to them\n",
    "\n",
    "for example\n",
    "\n",
    "```\n",
    "Users:\n",
    "- j.broeder@fz-juelich.de created 182 nodes\n",
    "- tests@aiida.mail created 104 nodes\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Users:\")\n",
    "helpers.print_Count('user',res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node types distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubtaskD1.c: Node types\n",
    "a = '''\n",
    "Task: plot node information in two pie chart plots\n",
    "\n",
    "One showing what data nodes there (with their lowest class names(node_type)) I.e Dict, K-pointsData, CifData, FleurinpData...\n",
    "\n",
    "And one chart showning the process nodes, (with their lowest class names(process_type) i.e CalcjobNodes: FleurCalcjob, FleurinputgenCalcjob, ...\n",
    "\n",
    "WorkChain nodes: FleurSCFWorkchain, FleurBandDosWorkchain, ..., calcfunctions, and workfunction nodes are fine to not show the lowest class names\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node types\n",
    "print(\"Node types:\")\n",
    "helpers.print_Count('types',res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data nodes and process nodes\n",
    "output_notebook()\n",
    "types = Counter([r[3] for r in res])\n",
    "x = helpers.get_data_node_count(types,'data') \n",
    "p = helpers.draw_pie_chart(x,'Data Nodes:%s')\n",
    "\n",
    "x1 = helpers.get_process_node_count(types,'process')\n",
    "p1 = helpers.draw_pie_chart(x1,'Process Nodes:%s')\n",
    "\n",
    "show(column(p,p1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "p = helpers.draw_pie_chart(Counter(helpers.get_dict_link_types()),'Dict Link Types:%s')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database time evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubtaskD1.d: Histogram\n",
    "# Task: Cumulative Histogram/ or line plot by ctime & mtime of all nodes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot by ctime & mtime\n",
    "users = Counter([r[4] for r in res])\n",
    "output_notebook()\n",
    "helpers.draw_line_plot(users,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubtaskD1.e: Codes\n",
    "#Task: List Code names, sorted by by how many calcjobs where run with each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = Code.objects.all()\n",
    "result = {code.full_label: len(code.get_outgoing(node_class=CalcJobNode).all_nodes()) for code in codes}\n",
    "#result_df=pd.Series(result).sort_values(ascending=False)\n",
    "result_df=pd.DataFrame({'code@computer':result.keys(),'CalaJobcount':result.values()}).sort_values(by='CalaJobcount',ascending=False).reset_index(drop=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubtaskD1.f: Groups\n",
    "#Task: List all group names with how many nodes they contain (verdi group list -C) (exclude import and export groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load_profile()\n",
    "#!verdi group list --all\n",
    "try:\n",
    "    Groups_data = SR.deserialize_from_file('./output/group.json',Node_type='Group')\n",
    "except:\n",
    "    qb = QueryBuilder()\n",
    "    qb.append(Group)\n",
    "    group = qb.all()\n",
    "\n",
    "    #data = GroupDataHelper(group)\n",
    "    #data.ListGroup(exclude=['export','import'])\n",
    "\n",
    "    ### add more columns for this and do also for other nodes\n",
    "    serializer = SR.Serializer(group)\n",
    "    serializer.to_file('./output/group.json',Node_type='Group')\n",
    "    Groups_data = SR.deserialize_from_file('./output/group.json',Node_type='Group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb = QueryBuilder()\n",
    "qb.append(Group)\n",
    "group = qb.all()\n",
    "group[0][0].__dict__\n",
    "s = dir(group[0][0])\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group[0][0].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Groups_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DV.GroupDataHelper(x)\n",
    "data.ListGroup(exclude=['export','import'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubtaskD1.g: Structures\n",
    "a = '''\n",
    "Task: Further analyze what structures are in the DB\n",
    "\n",
    "Number of structureData node versus how many atoms they contain. \n",
    "\n",
    "here interactive with bokeh hover tool showing the structure formula and uuid\n",
    "\n",
    "Number of StructureData nodes versus elements bokeh bar chart, since there are over \n",
    "100 elements in the periodic table you can split it over several plots, or just use the charge number as in \n",
    "'example/element_content.png' but then make it interactive that once one hovers \n",
    "with the mouse over a bar it tells you what element it is and how many structures there are containing this element-\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Newdata = SR.deserialize_from_file(filepath,Node_type = 'StructureFormula')\n",
    "except:\n",
    "    ################### serialization\n",
    "    qb = QueryBuilder()\n",
    "    qb.append(StructureData)\n",
    "    StructDatas = qb.all()\n",
    "\n",
    "    #print(dic.keys())\n",
    "\n",
    "    serializer = SR.Serializer(StructDatas)\n",
    "    filepath = './output/Num_structure.json'\n",
    "    serializer.to_file(filepath ,Node_type='StructureFormula')\n",
    "    Newdata = SR.deserialize_from_file(filepath,Node_type = 'StructureFormula')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DV.ShowFormula(Newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    x = SR.deserialize_from_file(filepath,'StructureElement')\n",
    "except:\n",
    "    qb = QueryBuilder()\n",
    "    qb.append(StructureData)\n",
    "    StructDatas = qb.all()\n",
    "    serializer = SR.Serializer(StructDatas)\n",
    "    filepath = './output/Struct_Element.json'\n",
    "    serializer.to_file(filepath,'StructureElement')\n",
    "    x = SR.deserialize_from_file(filepath,'StructureElement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShowElements(x)\n",
    "## sort in other ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubtaskD1.h: Calculations\n",
    "a = '''\n",
    "Task: more detail analysis of Calculations\n",
    "\n",
    "`print('\\n\\nMore detailed analysis of Calculations \\n')`\n",
    "\n",
    "List, stacked Histogram of Calculations types and the state it ended up finished, failed, exit codes, exit messages\n",
    "\n",
    "more detail analysis of WorkChains\n",
    "\n",
    "`print('\\n\\nMore detailed analysis of WorkChains \\n')`\n",
    "\n",
    "List,  stacked Histogram for each Workchain type and the state it ended up in finished, failed, exit codes, exit messages\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### CalcNode \n",
    "try:\n",
    "    filepath = './output/CalcNode.json'\n",
    "    calcArray = SR.deserialize_from_file(filepath,Node_type = 'ProcessNode')\n",
    "except:  \n",
    "    qb = QueryBuilder()\n",
    "    qb.append(CalcJobNode)\n",
    "    CalcNode = qb.all()\n",
    "\n",
    "    serializer = SR.Serializer(CalcNode)\n",
    "    filepath = './output/CalcNode.json'\n",
    "    serializer.to_file(filepath,'ProcessNode')\n",
    "    calcArray = SR.deserialize_from_file(filepath,Node_type = 'ProcessNode')\n",
    "\n",
    "######## WorkflowNode\n",
    "try:\n",
    "    filepath2 = './output/WorkflowNode.json'\n",
    "    WorkflowArray = SR.deserialize_from_file(filepath2,Node_type = 'ProcessNode')\n",
    "except:\n",
    "    qb = QueryBuilder()\n",
    "    qb.append(WorkflowNode)\n",
    "    WorkflowNodes = qb.all()\n",
    "\n",
    "    serializer = SR.Serializer(WorkflowNodes)\n",
    "    filepath2 = './output/WorkflowNode.json'\n",
    "    serializer.to_file(filepath2,'ProcessNode')\n",
    "    WorkflowArray = SR.deserialize_from_file(filepath2,Node_type = 'ProcessNode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcArray.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WorkflowArray.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newdict1 = DV.GetWorkflowDict(WorkflowArray)\n",
    "Newdict2 = DV.GetWorkflowDict(calcArray)\n",
    "DV.ShowWorkflow(Newdict1,'Work Flow Node Information')\n",
    "DV.ShowWorkflow(Newdict2,'Calculate Job Node Information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data provenance health indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SubtaskD1.i: Provenance\n",
    "#Task: Database and provenance health: display the number of nodes who have no incomming and outgoing links, no incomming links (any number outgoing), and no outgoing links (any number incomming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## this cell will take some time,but after the preprocessing everything should be fine\n",
    "try:\n",
    "    filepath = './output/provenance.json'\n",
    "    provenance = SR.deserialize_from_file(filepath,'Provenance')\n",
    "except:\n",
    "    qb = QueryBuilder()\n",
    "    qb.append(Node)\n",
    "    Nodes = qb.all()\n",
    "\n",
    "    #### serialization to filepath\n",
    "    provenance_serializer = SR.Serializer(Nodes)\n",
    "    filepath = './output/provenance.json'\n",
    "    provenance_serializer.to_file(filepath,'Provenance')\n",
    "    provenance = SR.deserialize_from_file(filepath,'Provenance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### deserialization from filepath\n",
    "provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "No_Incoming_Mydict,No_Outgoing_Mydict,No_InOut_Mydict = DV.Count_In_Out(provenance)\n",
    "print(No_Incoming_Mydict,No_Outgoing_Mydict,No_InOut_Mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DV.Show_In_Out(No_Incoming_Mydict,No_Outgoing_Mydict,No_InOut_Mydict)\n",
    "### split and think about bar plot\n",
    "# reduce complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}